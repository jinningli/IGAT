{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/path1/path2/ssbrl/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/path1/path2/ssbrl/venv/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Twitter-Roberta in eval mode\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model = RobertaModel.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embedding(tweet):\n",
    "    \"\"\"\n",
    "    Return the CLS embedding from the last hidden state of RoBERTa\n",
    "    \"\"\"\n",
    "    encoded_input = tokenizer(tweet, \n",
    "                              return_tensors='pt', \n",
    "                              truncation=True, \n",
    "                              max_length=512)\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input, output_hidden_states=True)\n",
    "        last_layer_hidden_states = output.hidden_states[-1]  # shape: (batch, seq_len, hidden_dim)\n",
    "        cls_embedding = last_layer_hidden_states[:, 0, :]    # shape: (batch, hidden_dim)\n",
    "    return cls_embedding.cpu().numpy().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/path1/path2/ssbrl/data/learning-to-slice/US_election_dataset.csv\"\n",
    "output_path = \"labled_data/kmeans_labelled.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "df = df.drop_duplicates(subset=\"index_text\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# Subset of labeled rows for discovering the best mapping\n",
    "df_labeled = df[df[\"is_gt\"] == 1].copy()\n",
    "if len(df_labeled) == 0:\n",
    "    raise ValueError(\"No labeled rows found (is_gt==1). Cannot find best mapping!\")\n",
    "\n",
    "df_labeled = df_labeled[df_labeled[\"manual_label\"] != \"neutral\"]\n",
    "\n",
    "# We assume df_labeled[\"manual_label_full\"] has exactly 4 possible classes\n",
    "unique_labels = df_labeled[\"manual_label_full\"].unique()\n",
    "unique_labels = sorted(unique_labels)  # ensure consistent order\n",
    "if len(unique_labels) != 4:\n",
    "    print(\"Warning: found these ground-truth labels in the labeled set:\", unique_labels)\n",
    "    print(\"But we are expecting exactly 4 distinct classes. Proceeding anyway...\")\n",
    "\n",
    "# Map label string -> integer index (0..3)\n",
    "label2idx = {lab: i for i, lab in enumerate(unique_labels)}\n",
    "idx2label = {v: k for k, v in label2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'opposing-Candidate_Advocacy': 0, 'opposing-Election_Legitimacy': 1, 'supportive-Candidate_Advocacy': 2, 'supportive-Election_Legitimacy': 3}\n",
      "{0: 'opposing-Candidate_Advocacy', 1: 'opposing-Election_Legitimacy', 2: 'supportive-Candidate_Advocacy', 3: 'supportive-Election_Legitimacy'}\n"
     ]
    }
   ],
   "source": [
    "# Inspections:\n",
    "print(label2idx)\n",
    "print(idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding all 4159 tweets...\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Embed ALL TWEETS\n",
    "# --------------------------\n",
    "print(f\"Embedding all {len(df)} tweets...\")\n",
    "start_time = time.time()\n",
    "embeddings = []\n",
    "for text in df[\"text\"]:\n",
    "    emb = get_cls_embedding(text)\n",
    "    embeddings.append(emb)\n",
    "embeddings_array = np.vstack(embeddings)  # shape: (N, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running KMeans(n_clusters=4)...\n",
      "Done embedding. Time: 22.62s\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# KMeans (4 clusters)\n",
    "# --------------------------\n",
    "print(\"Running KMeans(n_clusters=4)...\")\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "kmeans.fit(embeddings_array)\n",
    "end_time = time.time()\n",
    "print(f\"Done embedding. Time: {end_time - start_time:.2f}s\")\n",
    "cluster_ids = kmeans.labels_  # cluster assignments for each row in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store cluster in df\n",
    "df[\"cluster_id\"] = cluster_ids\n",
    "np.unique(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perm: (0, 1, 2, 3)\n",
      "----acc: 0.24\n",
      "perm: (0, 1, 3, 2)\n",
      "----acc: 0.22\n",
      "perm: (0, 2, 1, 3)\n",
      "----acc: 0.30333333333333334\n",
      "perm: (0, 2, 3, 1)\n",
      "----acc: 0.24\n",
      "perm: (0, 3, 1, 2)\n",
      "----acc: 0.29\n",
      "perm: (0, 3, 2, 1)\n",
      "----acc: 0.24666666666666667\n",
      "perm: (1, 0, 2, 3)\n",
      "----acc: 0.29\n",
      "perm: (1, 0, 3, 2)\n",
      "----acc: 0.27\n",
      "perm: (1, 2, 0, 3)\n",
      "----acc: 0.2866666666666667\n",
      "perm: (1, 2, 3, 0)\n",
      "----acc: 0.22666666666666666\n",
      "perm: (1, 3, 0, 2)\n",
      "----acc: 0.2733333333333333\n",
      "perm: (1, 3, 2, 0)\n",
      "----acc: 0.23333333333333334\n",
      "perm: (2, 0, 1, 3)\n",
      "----acc: 0.3\n",
      "perm: (2, 0, 3, 1)\n",
      "----acc: 0.23666666666666666\n",
      "perm: (2, 1, 0, 3)\n",
      "----acc: 0.23333333333333334\n",
      "perm: (2, 1, 3, 0)\n",
      "----acc: 0.17333333333333334\n",
      "perm: (2, 3, 0, 1)\n",
      "----acc: 0.24\n",
      "perm: (2, 3, 1, 0)\n",
      "----acc: 0.24333333333333335\n",
      "perm: (3, 0, 1, 2)\n",
      "----acc: 0.29333333333333333\n",
      "perm: (3, 0, 2, 1)\n",
      "----acc: 0.25\n",
      "perm: (3, 1, 0, 2)\n",
      "----acc: 0.22666666666666666\n",
      "perm: (3, 1, 2, 0)\n",
      "----acc: 0.18666666666666668\n",
      "perm: (3, 2, 0, 1)\n",
      "----acc: 0.24666666666666667\n",
      "perm: (3, 2, 1, 0)\n",
      "----acc: 0.25\n",
      "Best accuracy among labeled data = 0.3033\n",
      "Best cluster->label mapping: {0: 0, 1: 2, 2: 1, 3: 3}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "#  Find best permutation using ONLY the labeled subset\n",
    "# --------------------------\n",
    "# (1) We'll extract the cluster assignments for the labeled portion\n",
    "labeled_indices = df_labeled.index  # row indices in df\n",
    "labeled_clusters = df.loc[labeled_indices, \"cluster_id\"].values\n",
    "\n",
    "# (2) Convert ground-truth label -> label_idx\n",
    "df_labeled[\"label_idx\"] = df_labeled[\"manual_label_full\"].map(label2idx)\n",
    "labeled_gt = df_labeled[\"label_idx\"].values  # ground truth array (0..3)\n",
    "\n",
    "best_accuracy = -1.0\n",
    "best_mapping = {}\n",
    "\n",
    "# Permutations of [0,1,2,3]\n",
    "for perm in permutations(range(4)):\n",
    "    # clusterID -> label_idx\n",
    "    cluster2label = {cid: perm[cid] for cid in range(4)}\n",
    "\n",
    "    # Predicted label for the labeled portion\n",
    "    pred_label_idx = [cluster2label[c] for c in labeled_clusters]\n",
    "    \n",
    "    # Compute accuracy among labeled data\n",
    "    correct = sum(\n",
    "        1 for gt, pred in zip(labeled_gt, pred_label_idx) if gt == pred\n",
    "    )\n",
    "    accuracy = correct / len(labeled_gt)\n",
    "    print(f\"perm: {perm}\")\n",
    "    print(f\"----acc: {accuracy}\")\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_mapping = cluster2label\n",
    "\n",
    "print(f\"Best accuracy among labeled data = {best_accuracy:.4f}\")\n",
    "print(f\"Best cluster->label mapping: {best_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on labeled data = 0.3033\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "#  Final predictions for ALL data\n",
    "# --------------------------\n",
    "final_pred_idx = [best_mapping[c] for c in df[\"cluster_id\"]]\n",
    "pred_labels_str = [idx2label[i] for i in final_pred_idx]\n",
    "df[\"pred_label\"] = pred_labels_str\n",
    "\n",
    "# (Optional) measure final accuracy on the labeled subset (should match best_accuracy)\n",
    "labeled_pred_idx = [best_mapping[c] for c in labeled_clusters]\n",
    "correct_final = sum(1 for gt, pred in zip(labeled_gt, labeled_pred_idx) if gt == pred)\n",
    "final_accuracy = correct_final / len(labeled_gt)\n",
    "print(f\"Final accuracy on labeled data = {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final results (including pred_label) to labled_data/kmeans_labelled.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "#  Save results\n",
    "# --------------------------\n",
    "if output_path is not None:\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved final results (including pred_label) to {output_path}\")\n",
    "else:\n",
    "    print(\"No output_path given. Not saving CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
